{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Field Networks\n",
    "\n",
    "Implementation of shape classification demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from math import pi, sqrt\n",
    "import tensorfieldnetworks.utils as utils\n",
    "\n",
    "from tensorfieldnetworks.ShapeClassificationModel import ShapeClassificationModel\n",
    "\n",
    "tetris = [[(0, 0, 0), (0, 0, 1), (1, 0, 0), (1, 1, 0)],  # chiral_shape_1\n",
    "          [(0, 0, 0), (0, 0, 1), (1, 0, 0), (1, -1, 0)], # chiral_shape_2\n",
    "          [(0, 0, 0), (1, 0, 0), (0, 1, 0), (1, 1, 0)],  # square\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 0, 3)],  # line\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0)],  # corner\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 1, 0)],  # T\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 1, 1)],  # zigzag\n",
    "          [(0, 0, 0), (1, 0, 0), (1, 1, 0), (2, 1, 0)]]  # L\n",
    "\n",
    "dataset = [np.array(points_, dtype='float32') for points_ in tetris]\n",
    "num_classes = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[{0: [<tf.Tensor 'input:0' shape=(4, 1, 1) dtype=float32>]}, <tf.Tensor 'input_1:0' shape=(4, 4, 4) dtype=float32>, <tf.Tensor 'input_2:0' shape=(4, 4, 3) dtype=float32>]\ntest\n[{0: [<tf.Tensor 'input:0' shape=(4, 1, 1) dtype=float32>]}, <tf.Tensor 'input_1:0' shape=(4, 4, 4) dtype=float32>, <tf.Tensor 'input_2:0' shape=(4, 4, 3) dtype=float32>]\ntest\n[{0: [<tf.Tensor 'input:0' shape=(4, 4, 1) dtype=float32>], 1: [<tf.Tensor 'input_1:0' shape=(4, 4, 3) dtype=float32>]}, <tf.Tensor 'input_2:0' shape=(4, 4, 4) dtype=float32>, <tf.Tensor 'input_3:0' shape=(4, 4, 3) dtype=float32>]\ntest\n[{0: [<tf.Tensor 'input:0' shape=(4, 4, 1) dtype=float32>], 1: [<tf.Tensor 'input_1:0' shape=(4, 4, 3) dtype=float32>]}, <tf.Tensor 'input_2:0' shape=(4, 4, 4) dtype=float32>, <tf.Tensor 'input_3:0' shape=(4, 4, 3) dtype=float32>]\ntest\n[{0: [<tf.Tensor 'input:0' shape=(4, 4, 1) dtype=float32>], 1: [<tf.Tensor 'input_1:0' shape=(4, 4, 3) dtype=float32>]}, <tf.Tensor 'input_2:0' shape=(4, 4, 4) dtype=float32>, <tf.Tensor 'input_3:0' shape=(4, 4, 3) dtype=float32>]\ntest\n[{0: [<tf.Tensor 'input:0' shape=(4, 4, 1) dtype=float32>], 1: [<tf.Tensor 'input_1:0' shape=(4, 4, 3) dtype=float32>]}, <tf.Tensor 'input_2:0' shape=(4, 4, 4) dtype=float32>, <tf.Tensor 'input_3:0' shape=(4, 4, 3) dtype=float32>]\ntest\nModel: \"shape_classification_model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nself_interaction_simple_7 (S multiple                  1         \n_________________________________________________________________\ninput_layer_1 (InputLayer)   multiple                  0         \n_________________________________________________________________\nconvolution_layer_3 (Convolu multiple                  50        \n_________________________________________________________________\nconcatenation_layer_3 (Conca multiple                  0         \n_________________________________________________________________\nself_interaction_layer_3 (Se multiple                  8         \n_________________________________________________________________\nnonlinearity_layer_3 (Nonlin multiple                  4         \n_________________________________________________________________\nconvolution_layer_4 (Convolu multiple                  200       \n_________________________________________________________________\nconcatenation_layer_4 (Conca multiple                  0         \n_________________________________________________________________\nself_interaction_layer_4 (Se multiple                  80        \n_________________________________________________________________\nnonlinearity_layer_4 (Nonlin multiple                  4         \n_________________________________________________________________\nconvolution_layer_5 (Convolu multiple                  200       \n_________________________________________________________________\nconcatenation_layer_5 (Conca multiple                  0         \n_________________________________________________________________\nself_interaction_layer_5 (Se multiple                  80        \n_________________________________________________________________\nnonlinearity_layer_5 (Nonlin multiple                  4         \n_________________________________________________________________\noutput_layer_1 (OutputLayer) multiple                  40        \n=================================================================\nTotal params: 671\nTrainable params: 671\nNon-trainable params: 0\n_________________________________________________________________\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(8,), dtype=float32, numpy=\narray([ 0.7116172 ,  0.66414523, -0.04071363, -0.03242678, -0.5528971 ,\n       -0.3408018 , -0.36797613,  0.3079396 ], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "model = ShapeClassificationModel(num_classes)\n",
    "model(dataset[0])\n",
    "model.summary()\n",
    "model(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1.e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0: validation loss = 0.089\nEpoch 10: validation loss = 0.089\nEpoch 20: validation loss = 0.089\nEpoch 30: validation loss = 0.089\nEpoch 40: validation loss = 0.089\nEpoch 50: validation loss = 0.089\nEpoch 60: validation loss = 0.089\nEpoch 70: validation loss = 0.089\nEpoch 80: validation loss = 0.089\nEpoch 90: validation loss = 0.089\nEpoch 100: validation loss = 0.089\nEpoch 110: validation loss = 0.089\nEpoch 120: validation loss = 0.089\nEpoch 130: validation loss = 0.089\nEpoch 140: validation loss = 0.089\nEpoch 150: validation loss = 0.089\nEpoch 160: validation loss = 0.089\nEpoch 170: validation loss = 0.089\nEpoch 180: validation loss = 0.089\nEpoch 190: validation loss = 0.089\n"
    }
   ],
   "source": [
    "max_epochs = 200\n",
    "print_freq = 10\n",
    "\n",
    "#with tf.profiler.experimental.Profile('logdir'):\n",
    "for epoch in range(max_epochs):    \n",
    "    loss_sum = 0.\n",
    "    for label, shape in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = model(shape, training=True)\n",
    "            truth = tf.one_hot(label, num_classes)\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(labels=truth, logits=pred)\n",
    "            loss_sum += loss\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n",
    "        \n",
    "    if epoch % print_freq == 0:\n",
    "        print(\"Epoch %d: validation loss = %.3f\" % (epoch, loss_sum / num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 4, 4, 4, 4, 4, 4, 4, 4\n  y sizes: 8, 8, 8, 8, 8, 8, 8, 8\nPlease provide data which shares the same first dimension.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d4cc11171d91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    800\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m    803\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[1;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 4, 4, 4, 4, 4, 4, 4, 4\n  y sizes: 8, 8, 8, 8, 8, 8, 8, 8\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(dataset, [tf.one_hot(label, num_classes) for label in range(num_classes)], epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer shape_classification_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nTest accuracy: 1.000000\n"
    }
   ],
   "source": [
    "rng = np.random.RandomState()\n",
    "test_set_size = 25\n",
    "predictions = [list() for i in range(len(dataset))]\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "for i in range(test_set_size):\n",
    "    for label, shape in enumerate(dataset):\n",
    "        rotation = utils.random_rotation_matrix(rng)\n",
    "        rotated_shape = np.dot(shape, rotation)\n",
    "        translation = np.expand_dims(np.random.uniform(low=-3., high=3., size=(3)), axis=0)\n",
    "        translated_shape = rotated_shape + translation\n",
    "        output_label = tf.argmax(model(rotated_shape))\n",
    "        total_predictions += 1\n",
    "        if output_label == label:\n",
    "            correct_predictions += 1\n",
    "print('Test accuracy: %f' % (float(correct_predictions) / total_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}